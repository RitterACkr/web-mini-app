## モデル改善

---

## 1. モデルアーキテクチャ

### ネットワーク構造

```
入力層 (192次元) → 隠れ層1 (128Unit, ReLU) → 隠れ層2 (64Unit, ReLU) → 出力層 (64次元, Linear)
```

入力の192次元は以下の3チャンネルを64マス分フラット化したもの
- 自分の石 (64次元)
- 相手の石 (64次元)
- 打てる手のマスク (64次元)

出力の64次元は盤面全マスのQ値．打てる手の中でQ値最大の手を選択する

---

## 2. モデル改善

### Step 1: 初期実装
最初の実装. 同一モデルで黒・白の両方の手番を担当する1モデル構成

| 項目 | 内容 |
|----|----|
| 構成 | 1モデルで黒白両方を担当 |
| 問題点 | 色の偏り・学習が不安定 |
| 行動選択 | ε-greedy (ε=0.3) |

---

### Step 2: 色の入れ替え & Snapshotモデルの導入
メインモデル (学習側) と スナップショットモデル (対戦相手側) の2モデル構成に変更

| 項目 | 内容 |
|----|----|
| 構成 | メインモデル vs スナップショットモデル |
| snapshot更新間隔 | N epoch毎 (UIで設定可能) |
| 効果 | Step 1よりすこし改善 |

epochごとにメインモデルが担当する色を交互に入れ替えることで色の偏りを解消

| epoch | メインモデルの色 |
|----|----|
| 偶数epoch | BLACK |
| 奇数epoch | WHITE |

---

### Step 3: Monte Carlo報酬方式 & 中間報酬(石の数差)の追加 & Experience Replayの導入

#### Monte Carlo報酬方式
当初はDQN (Bellman方程式) を使っていたが,nextBoardが相手ターンの盤面になるためQ値計算にノイズが発生していた.
最終勝敗のみを用いたMonte Carlo方式に変更し,時系列を逆順に遡りながらγで割引いて伝播する方式に修正.

| 変更前 | 変更後 |
|----|----|
| DQN (Bellman方程式) | Monte Carlo (累積割引報酬) |
| nextBoardのQ値を使用 | 最終勝敗のみから逆伝播 |

#### 中間報酬(石の数差)の追加
最終勝敗に関係なく各手での石の差の変化量を中間報酬として学習に組み込んだ

```
combinedG = 最終報酬 * γ ^ stepFromEnd + 石差変化量 / 64 * 0.3
```

| 報酬の種類 | 内容 |
|----|----|
| 最終報酬 | 勝ち = +1.0 / 引き分け = +0.1 / 負け = -1.0 |
| 中間報酬 | 石差の変化量 / 64 (重み 0.3) |

#### Experience Replayの導入
1ゲームずつ学習する方式から, 過去の対局をバッファに溜めてランダムサンプリングでバッチ学習する方式に変更

| 項目 | 内容 | 
|----|----|
| バッファサイズ | 最大2000件 (古いものを削除) |
| バッチサイズ | 32件 (バッファが32件以上で学習開始) |
| サンプリング | ランダムシャッフル後に先頭32件 |
| 効果 | Step 2より大いに改善 |


## 3. 今後の展望
- ネットワークを深くする
  - 隠れ層の追加
  - ユニット数増加